<!DOCTYPE html>
<html lang="en">

<head>
        <meta charset="utf-8">
        <!--Bootstrap stuff-->
        <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

        <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous">
        <script src="https://code.jquery.com/jquery-3.3.1.slim.min.js" integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo" crossorigin="anonymous"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.7/umd/popper.min.js" integrity="sha384-UO2eT0CpHqdSJQ6hJty5KVphtPhzWj9WO1clHTMGa3JDZwrnQq4sF86dIHNDz0W1" crossorigin="anonymous"></script>
        <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.min.js" integrity="sha384-JjSmVgyd0p3pXB1rRibZUAYoIIy6OrQ6VrjIEaFf/nJGzIxFDsf4x0xIM+B07jRM" crossorigin="anonymous"></script>


        <link rel="stylesheet" href="../styles/styleProjects.css">
        <link rel="stylesheet" href="../styles/contact.css">
        <link rel="stylesheet" href="../styles/projectNavigation.css">
        <link rel="stylesheet" href="../styles/navbar.css">


</head>

<body>
    <div id = "wrapper">

        <div id = "contact">
                <ul>
                    <li><a href = "http://github.com/douglasliralima" target="_blank"><img class = "icon" src = "../Assets/github.png"></a></li>
                    <li><a href = "https://www.linkedin.com/in/douglasliralima/" target="_blank"><img class = "icon" src = "../Assets/linkedin.png"></a></li>
                </ul>    
        </div>

        <div id = "project-navigation">
                <div id = "navbar">
                        <ul>
                            <li><span><a href = "../index.html">Home</a> > Libras Translator</span></li>
                        </ul>    
                </div>
                <ul>
                    <li id = "previous-project">
                        <a href = "InventoryManagementSystem.html"><img class = "icon" src = "../Assets/previous.png"></a>
                        <span>Inventory Management System </span>
                    </li>
                    <li id = "next-project">
                        <span>Dungeon Scrolls</span>
                        <a href = "DungeonScrolls.html"><img class = "icon" src = "../Assets/next.png"></a>
                    </li>
                </ul>    
        </div>

        <h1>Libras Translator</h1><br>
        <span>Codigo Fonte : <a href = "https://github.com/douglasliralima/Sign4Text" target = "_blank">Publico</a></span>
        <br><br>
        <p>
            Projeto de pesquisa desenvolvido no laboratório de vídeo para a tradução 
            de sinais de Libras(Linguagem brasileira de sinais) para linguagem natural
            usando redes neurais convolucionais.

        </p>
        <h2>Contributions</h2>
        <hr>

        <p>
            Nesse projeto, eu fiquei responsável do preprocessamento de dados, desde o reshape
            da imagem, carregamento estratificado da base, até a alimentação das arquiteturas
            estado da arte de redes neurais.
        </p>

        <h2>Criação da base</h2>
        <hr>

        <p>
            Initially, we create a dataset of videos that aims to get closer to real
            practical situations, to build this, initially we recorded videos for each of the 26 letters
            of the alphabet in Libras with 13 users considering different environments, 
            backgrounds, signers, body, arms and hand positions, lighting patterns, age, 
            gender and ethnicity, which results in 338 recorded videos.<br><br>
            Afterwards, we preprocess the videos with the FFmpeg software, extracting 
            20 images per second of the videos collected. Some examples of these images are 
            bellow:
            <br><br><i>imagens</i><br><br>
        </p>

        <h2>Preprocessamento</h2>
        <hr>

        <p>

            Afterward, we apply a data augmentation in these images/samples, using Keras. The parameters 
            applied in this data augmentation were as follows:

            <br><lu>
                <li>Zoom factor of  0.2 at maximum;</li>
                <li>Increase/Decrease of illumination with a range maximum of 30% of the original illumination;</li>
                <li>A shear factor of 0.8 at maximum;</li>
                <li>Rotation with a range of 0 to a maximum of 10º in clockwise or counterclockwise of the original image;</li>
                <li>Mirroring the samples.</li>
            </lu><br>
                
            <i>imagens</i><br>
            Figure 2: Some examples of images of the dataset after applying data augmentation.
            <br><br>
            
            The resulting final dataset consists of approximately 224000 images, 
            from the 26 letters of the Libras alphabet signed by 13 persons in 
            different backgrounds, body arm, and hand positions, and different lighting 
            patterns were generated.
            <br><br><i>imagens</i><br><br>
        </p>

        <h2>Pipeline de dados</h2>
        <hr>

        <p>
            Before training, we also use a stratified collection of samples in batches,
            due to the difficulty in managing the use of memory in the platform that
            we use for training and testing. More specifically, for each of the 26 
            letters of the alphabet, the largest possible number of images were taken,
            which the memory of the training machine would be able to support, leaving only 
            free space for the processing of main functions of the operating system, and other 
            operations. <br><br>

            For example, for the VGG16 model, we were only able to run 44,800 instances (images) 
            per batch. Thus, we had to execute five batches of 44,800 images to complete one 
            eppoch (224 thousand images).            
            <br><br><i>imagem</i>
        </p>


        <h2>Treinamento</h2>
        <hr>

        <p>
            Since our final goal is a solution that can be used by Brazilian deaf people in 
            real situations and ubiquitously, we focus on the use of neural network architectures 
            that were efficient for use in smartphones or embedded systems, but which also 
            presented good results in competitions or in works in other areas of computer 
            vision, such as the  MobileNet, with 71% in ImageNet and NASNetMobile, which 
            was proposed more recently and has better results in ImageNet (74.1%) when 
            compared to MobileNet. However, for comparison, we also perform tests 
            on more robust architectures like VGG16, InceptionResNetV2, respectively 
            with accuracy in ImageNet of 71% and 80% in NasNetLarge which is also more 
            recent than the other two cited and achieved state of the art accuracy of 83% 
            in the same dataset.
            <br><br><i>imagem</i><br><br>
        </p>
    </div>
        

</body>

</html>